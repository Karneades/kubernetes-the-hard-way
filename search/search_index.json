{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Introduction","text":"<p>This tutorial walks you through setting up Kubernetes the hard way. This guide is not for someone looking for a fully automated tool to bring up a Kubernetes cluster. Kubernetes The Hard Way is optimized for learning, which means taking the long route to ensure you understand each task required to bootstrap a Kubernetes cluster.</p> <p>The results of this tutorial should not be viewed as production ready, and may receive limited support from the community, but don't let that stop you from learning!</p>"},{"location":"#copyright","title":"Copyright","text":"<p>This work is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License.</p>"},{"location":"#target-audience","title":"Target Audience","text":"<p>The target audience for this tutorial is someone who wants to understand the fundamentals of Kubernetes and how the core components fit together.</p>"},{"location":"#cluster-details","title":"Cluster Details","text":"<p>Kubernetes The Hard Way guides you through bootstrapping a basic Kubernetes cluster with all control plane components running on a single node, and two worker nodes, which is enough to learn the core concepts.</p> <p>Component versions:</p> <ul> <li>kubernetes v1.28.x</li> <li>containerd v1.7.x</li> <li>cni v1.3.x</li> <li>etcd v3.4.x</li> </ul>"},{"location":"#labs","title":"Labs","text":"<p>This tutorial requires four (4) ARM64 based virtual or physical machines connected to the same network. While ARM64 based machines are used for the tutorial, the lessons learned can be applied to other platforms.</p>"},{"location":"01-prerequisites/","title":"Prerequisites","text":"<p>In this lab you will review the machine requirements necessary to follow this tutorial.</p>"},{"location":"01-prerequisites/#virtual-or-physical-machines","title":"Virtual or Physical Machines","text":"<p>This tutorial requires four (4) virtual or physical ARM64 machines running Debian 12 (bookworm). The follow table list the four machines and thier CPU, memory, and storage requirements.</p> Name Description CPU RAM Storage jumpbox Administration host 1 512MB 10GB server Kubernetes server 1 2GB 20GB node-0 Kubernetes worker node 1 2GB 20GB node-1 Kubernetes worker node 1 2GB 20GB <p>How you provision the machines is up to you, the only requirement is that each machine meet the above system requirements including the machine specs and OS version. Once you have all four machine provisioned, verify the system requirements by running the <code>uname</code> command on each machine:</p> <pre><code>uname -mov\n</code></pre> <p>After running the <code>uname</code> command you should see the following output:</p> <pre><code>#1 SMP Debian 6.1.55-1 (2023-09-29) aarch64 GNU/Linux\n</code></pre> <p>You maybe surprised to see <code>aarch64</code> here, but that is the official name for the Arm Architecture 64-bit instruction set. You will often see <code>arm64</code> used by Apple, and the maintainers of the Linux kernel, when referring to support for <code>aarch64</code>. This tutorial will use <code>arm64</code> consistently throughout to avoid confusion.</p> <p>Next: setting-up-the-jumpbox</p>"},{"location":"02-jumpbox/","title":"Set Up The Jumpbox","text":"<p>In this lab you will set up one of the four machines to be a <code>jumpbox</code>. This machine will be used to run commands in this tutorial. While a dedicated machine is being used to ensure consistency, these commands can also be run from just about any machine including your personal workstation running macOS or Linux.</p> <p>Think of the <code>jumpbox</code> as the administration machine that you will use as a home base when setting up your Kubernetes cluster from the ground up. One thing we need to do before we get started is install a few command line utilities and clone the Kubernetes The Hard Way git repository, which contains some additional configuration files that will be used to configure various Kubernetes components throughout this tutorial. </p> <p>Log in to the <code>jumpbox</code>:</p> <pre><code>ssh root@jumpbox\n</code></pre> <p>All commands will be run as the <code>root</code> user. This is being done for the sake of convenience, and will help reduce the number of commands required to set everything up.</p>"},{"location":"02-jumpbox/#install-command-line-utilities","title":"Install Command Line Utilities","text":"<p>Now that you are logged into the <code>jumpbox</code> machine as the <code>root</code> user, you will install the command line utilities that will be used to preform various tasks throughout the tutorial. </p> <pre><code>apt-get -y install wget curl vim openssl git\n</code></pre>"},{"location":"02-jumpbox/#sync-github-repository","title":"Sync GitHub Repository","text":"<p>Now it's time to download a copy of this tutorial which contains the configuration files and templates that will be used build your Kubernetes cluster from the ground up. Clone the Kubernetes The Hard Way git repository using the <code>git</code> command:</p> <pre><code>git clone --depth 1 \\\n  https://github.com/kelseyhightower/kubernetes-the-hard-way.git\n</code></pre> <p>Change into the <code>kubernetes-the-hard-way</code> directory:</p> <pre><code>cd kubernetes-the-hard-way\n</code></pre> <p>This will be the working directory for the rest of the tutorial. If you ever get lost run the <code>pwd</code> command to verify you are in the right directory when running commands on the <code>jumpbox</code>:</p> <pre><code>pwd\n</code></pre> <pre><code>/root/kubernetes-the-hard-way\n</code></pre>"},{"location":"02-jumpbox/#download-binaries","title":"Download Binaries","text":"<p>In this section you will download the binaries for the various Kubernetes components. The binaries will be stored in the <code>downloads</code> directory on the <code>jumpbox</code>, which will reduce the amount of internet bandwidth required to complete this tutorial as we avoid downloading the binaries multiple times for each machine in our Kubernetes cluster.</p> <p>From the <code>kubernetes-the-hard-way</code> directory create a <code>downloads</code> directory using the <code>mkdir</code> command:</p> <pre><code>mkdir downloads\n</code></pre> <p>The binaries that will be downloaded are listed in the <code>downloads.txt</code> file, which you can review using the <code>cat</code> command:</p> <pre><code>cat downloads.txt\n</code></pre> <p>Download the binaries listed in the <code>downloads.txt</code> file using the <code>wget</code> command:</p> <pre><code>wget -q --show-progress \\\n  --https-only \\\n  --timestamping \\\n  -P downloads \\\n  -i downloads.txt\n</code></pre> <p>Depending on your internet connection speed it may take a while to download the <code>584</code> megabytes of binaries, and once the download is complete, you can list them using the <code>ls</code> command:</p> <pre><code>ls -loh downloads\n</code></pre> <pre><code>total 584M\n-rw-r--r-- 1 root  41M May  9 13:35 cni-plugins-linux-arm64-v1.3.0.tgz\n-rw-r--r-- 1 root  34M Oct 26 15:21 containerd-1.7.8-linux-arm64.tar.gz\n-rw-r--r-- 1 root  22M Aug 14 00:19 crictl-v1.28.0-linux-arm.tar.gz\n-rw-r--r-- 1 root  15M Jul 11 02:30 etcd-v3.4.27-linux-arm64.tar.gz\n-rw-r--r-- 1 root 111M Oct 18 07:34 kube-apiserver\n-rw-r--r-- 1 root 107M Oct 18 07:34 kube-controller-manager\n-rw-r--r-- 1 root  51M Oct 18 07:34 kube-proxy\n-rw-r--r-- 1 root  52M Oct 18 07:34 kube-scheduler\n-rw-r--r-- 1 root  46M Oct 18 07:34 kubectl\n-rw-r--r-- 1 root 101M Oct 18 07:34 kubelet\n-rw-r--r-- 1 root 9.6M Aug 10 18:57 runc.arm64\n</code></pre>"},{"location":"02-jumpbox/#install-kubectl","title":"Install kubectl","text":"<p>In this section you will install the <code>kubectl</code>, the official Kubernetes client command line tool, on the <code>jumpbox</code> machine. `kubectl will be used to interact with the Kubernetes control once your cluster is provisioned later in this tutorial.</p> <p>Use the <code>chmod</code> command to make the <code>kubectl</code> binary executable and move it to the <code>/usr/local/bin/</code> directory:</p> <pre><code>{\n  chmod +x downloads/kubectl\n  cp downloads/kubectl /usr/local/bin/\n}\n</code></pre> <p>At this point <code>kubectl</code> is installed and can be verified by running the <code>kubectl</code> command:</p> <pre><code>kubectl version --client\n</code></pre> <pre><code>Client Version: v1.28.3\nKustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3\n</code></pre> <p>At this point the <code>jumpbox</code> has been set up with all the command line tools and utilities necessary to complete the labs in this tutorial.</p> <p>Next: Provisioning Compute Resources</p>"},{"location":"03-compute-resources/","title":"Provisioning Compute Resources","text":"<p>Kubernetes requires a set of machines to host the Kubernetes control plane and the worker nodes where containers are ultimately run. In this lab you will provision the machines required for setting up a Kubernetes cluster.</p>"},{"location":"03-compute-resources/#machine-database","title":"Machine Database","text":"<p>This tutorial will leverage a text file, which will serve as a machine database, to store the various machine attributes that will be used when setting up the Kubernetes control plane and worker nodes. The following schema represents entries in the machine database, one entry per line:</p> <pre><code>IPV4_ADDRESS FQDN HOSTNAME POD_SUBNET\n</code></pre> <p>Each of the columns corresponds to a machine IP address <code>IPV4_ADDRESS</code>, fully qualified domain name <code>FQDN</code>, host name <code>HOSTNAME</code>, and the IP subnet <code>POD_SUBNET</code>. Kubernetes assigns one IP address per <code>pod</code> and the <code>POD_SUBNET</code> represents the unique IP address range assigned to each machine in the cluster for doing so.  </p> <p>Here is an example machine database similar to the one used when creating this tutorial. Notice the IP addresses have been masked out. Your machines can be assigned any IP address as long as each machine is reachable from each other and the <code>jumpbox</code>.</p> <pre><code>cat machines.txt\n</code></pre> <pre><code>XXX.XXX.XXX.XXX server.kubernetes.local server  \nXXX.XXX.XXX.XXX node-0.kubernetes.local node-0 10.200.0.0/24\nXXX.XXX.XXX.XXX node-1.kubernetes.local node-1 10.200.1.0/24\n</code></pre> <p>Now it's your turn to create a <code>machines.txt</code> file with the details for the three machines you will be using to create your Kubernetes cluster. Use the example machine database from above and add the details for your machines. </p>"},{"location":"03-compute-resources/#configuring-ssh-access","title":"Configuring SSH Access","text":"<p>SSH will be used to configure the machines in the cluster. Verify that you have <code>root</code> SSH access to each machine listed in your machine database. You may need to enable root SSH access on each node by updating the sshd_config file and restarting the SSH server.</p>"},{"location":"03-compute-resources/#enable-root-ssh-access","title":"Enable root SSH Access","text":"<p>If <code>root</code> SSH access is enabled for each of your machines you can skip this section.</p> <p>By default, a new <code>debian</code> install disables SSH access for the <code>root</code> user. This is done for security reasons as the <code>root</code> user is a well known user on Linux systems, and if a weak password is used on a machine connected to the internet, well, let's just say it's only a matter of time before your machine belongs to someone else. As mention earlier, we are going to enable <code>root</code> access over SSH in order to streamline the steps in this tutorial. Security is a tradeoff, and in this case, we are optimizing for convenience. On each machine login via SSH using your user account, then switch to the <code>root</code> user using the <code>su</code> command:</p> <pre><code>su - root\n</code></pre> <p>Edit the <code>/etc/ssh/sshd_config</code> SSH daemon configuration file and the <code>PermitRootLogin</code> option to <code>yes</code>:</p> <pre><code>sed -i \\\n  's/^#PermitRootLogin.*/PermitRootLogin yes/' \\\n  /etc/ssh/sshd_config\n</code></pre> <p>Restart the <code>sshd</code> SSH server to pick up the updated configuration file:</p> <pre><code>systemctl restart sshd\n</code></pre>"},{"location":"03-compute-resources/#generate-and-distribute-ssh-keys","title":"Generate and Distribute SSH Keys","text":"<p>In this section you will generate and distribute an SSH keypair to the <code>server</code>, <code>node-0</code>, and <code>node-1</code>, machines, which will be used to run commands on those machines throughout this tutorial. Run the following commands from the <code>jumpbox</code> machine.</p> <p>Generate a new SSH key:</p> <pre><code>ssh-keygen\n</code></pre> <pre><code>Generating public/private rsa key pair.\nEnter file in which to save the key (/root/.ssh/id_rsa): \nEnter passphrase (empty for no passphrase): \nEnter same passphrase again: \nYour identification has been saved in /root/.ssh/id_rsa\nYour public key has been saved in /root/.ssh/id_rsa.pub\n</code></pre> <p>Copy the SSH public key to each machine:</p> <pre><code>while read IP FQDN HOST SUBNET; do \n  ssh-copy-id root@${IP}\ndone &lt; machines.txt\n</code></pre> <p>Once each key is added, verify SSH public key access is working:</p> <pre><code>while read IP FQDN HOST SUBNET; do \n  ssh -n root@${IP} uname -o -m\ndone &lt; machines.txt\n</code></pre> <pre><code>aarch64 GNU/Linux\naarch64 GNU/Linux\naarch64 GNU/Linux\n</code></pre>"},{"location":"03-compute-resources/#hostnames","title":"Hostnames","text":"<p>In this section you will assign hostnames to the <code>server</code>, <code>node-0</code>, and <code>node-1</code> machines. The hostname will be used when executing commands from the <code>jumpbox</code> to each machine. The hostname also play a major role within the cluster. Instead of Kubernetes clients using an IP address to issue commands to the Kubernetes API server, those client will use the <code>server</code> hostname instead. Hostnames are also used by each worker machine, <code>node-0</code> and <code>node-1</code> when registering with a given Kubernetes cluster.</p> <p>To configure the hostname for each machine, run the following commands on the <code>jumpbox</code>.</p> <p>Set the hostname on each machine listed in the <code>machines.txt</code> file:</p> <pre><code>while read IP FQDN HOST SUBNET; do \n    CMD=\"sed -i 's/^127.0.1.1.*/127.0.1.1\\t${FQDN} ${HOST}/' /etc/hosts\"\n    ssh -n root@${IP} \"$CMD\"\n    ssh -n root@${IP} hostnamectl hostname ${HOST}\ndone &lt; machines.txt\n</code></pre> <p>Verify the hostname is set on each machine:</p> <pre><code>while read IP FQDN HOST SUBNET; do\n  ssh -n root@${IP} hostname --fqdn\ndone &lt; machines.txt\n</code></pre> <pre><code>server.kubernetes.local\nnode-0.kubernetes.local\nnode-1.kubernetes.local\n</code></pre>"},{"location":"03-compute-resources/#dns","title":"DNS","text":"<p>In this section you will generate a DNS <code>hosts</code> file which will be appended to <code>jumpbox</code> local <code>/etc/hosts</code> file and to the <code>/etc/hosts</code> file of all three machines used for this tutorial. This will allow each machine to be reachable using a hostname such as <code>server</code>, <code>node-0</code>, or <code>node-1</code>.</p> <p>Create a new <code>hosts</code> file and add a header to identify the machines being added:</p> <pre><code>echo \"\" &gt; hosts\necho \"# Kubernetes The Hard Way\" &gt;&gt; hosts\n</code></pre> <p>Generate a DNS entry for each machine in the <code>machines.txt</code> file and append it to the <code>hosts</code> file:</p> <pre><code>while read IP FQDN HOST SUBNET; do \n    ENTRY=\"${IP} ${FQDN} ${HOST}\"\n    echo $ENTRY &gt;&gt; hosts\ndone &lt; machines.txt\n</code></pre> <p>Review the DNS entries in the <code>hosts</code> file:</p> <pre><code>cat hosts\n</code></pre> <pre><code># Kubernetes The Hard Way\nXXX.XXX.XXX.XXX server.kubernetes.local server\nXXX.XXX.XXX.XXX node-0.kubernetes.local node-0\nXXX.XXX.XXX.XXX node-1.kubernetes.local node-1\n</code></pre>"},{"location":"03-compute-resources/#adding-dns-entries-to-a-local-machine","title":"Adding DNS Entries To A Local Machine","text":"<p>In this section you will append the DNS entries from the <code>hosts</code> file to the local <code>/etc/hosts</code> file on your <code>jumpbox</code> machine.</p> <p>Append the DNS entries from <code>hosts</code> to <code>/etc/hosts</code>:</p> <pre><code>cat hosts &gt;&gt; /etc/hosts\n</code></pre> <p>Verify that the <code>/etc/hosts</code> file has been updated:</p> <pre><code>cat /etc/hosts\n</code></pre> <pre><code>127.0.0.1       localhost\n127.0.1.1       jumpbox\n\n# The following lines are desirable for IPv6 capable hosts\n::1     localhost ip6-localhost ip6-loopback\nff02::1 ip6-allnodes\nff02::2 ip6-allrouters\n\n\n\n# Kubernetes The Hard Way\nXXX.XXX.XXX.XXX server.kubernetes.local server\nXXX.XXX.XXX.XXX node-0.kubernetes.local node-0\nXXX.XXX.XXX.XXX node-1.kubernetes.local node-1\n</code></pre> <p>At this point you should be able to SSH to each machine listed in the <code>machines.txt</code> file using a hostname.</p> <pre><code>for host in server node-0 node-1\n   do ssh root@${host} uname -o -m -n\ndone\n</code></pre> <pre><code>server aarch64 GNU/Linux\nnode-0 aarch64 GNU/Linux\nnode-1 aarch64 GNU/Linux\n</code></pre>"},{"location":"03-compute-resources/#adding-dns-entries-to-the-remote-machines","title":"Adding DNS Entries To The Remote Machines","text":"<p>In this section you will append the DNS entries from <code>hosts</code> to <code>/etc/hosts</code> on each machine listed in the <code>machines.txt</code> text file.</p> <p>Copy the <code>hosts</code> file to each machine and append the contents to <code>/etc/hosts</code>:</p> <pre><code>while read IP FQDN HOST SUBNET; do\n  scp hosts root@${HOST}:~/\n  ssh -n \\\n    root@${HOST} \"cat hosts &gt;&gt; /etc/hosts\"\ndone &lt; machines.txt\n</code></pre> <p>At this point hostnames can be used when connecting to machines from your <code>jumpbox</code> machine, or any of the three machines in the Kubernetes cluster. Instead of using IP addresess you can now connect to machines using a hostname such as <code>server</code>, <code>node-0</code>, or <code>node-1</code>.</p> <p>Next: Provisioning a CA and Generating TLS Certificates</p>"},{"location":"04-certificate-authority/","title":"Provisioning a CA and Generating TLS Certificates","text":"<p>In this lab you will provision a PKI Infrastructure using openssl to bootstrap a Certificate Authority, and generate TLS certificates for the following components: kube-apiserver, kube-controller-manager, kube-scheduler, kubelet, and kube-proxy. The commands in this section should be run from the <code>jumpbox</code>.</p>"},{"location":"04-certificate-authority/#certificate-authority","title":"Certificate Authority","text":"<p>In this section you will provision a Certificate Authority that can be used to generate additional TLS certificates for the other Kubernetes components. Setting up CA and generating certificates using <code>openssl</code> can be time-consuming, especially when doing it for the first time. To streamline this lab, I've included an openssl configuration file <code>ca.conf</code>, which defines all the details needed to generate certificates for each Kubernetes component. </p> <p>Take a moment to review the <code>ca.conf</code> configuration file:</p> <pre><code>cat ca.conf\n</code></pre> <p>You don't need to understand everything in the <code>ca.conf</code> file to complete this tutorial, but you should consider it a starting point for learning <code>openssl</code> and the configuration that goes into managing certificates at a high level.</p> <p>Every certificate authority starts with a private key and root certificate. In this section we are going to create a self-signed certificate authority, and while that's all we need for this tutorial, this shouldn't be considered something you would do in a real-world production level environment. </p> <p>Generate the CA configuration file, certificate, and private key:</p> <pre><code>{\n  openssl genrsa -out ca.key 4096\n  openssl req -x509 -new -sha512 -noenc \\\n    -key ca.key -days 3653 \\\n    -config ca.conf \\\n    -out ca.crt\n}\n</code></pre> <p>Results:</p> <pre><code>ca.crt ca.key\n</code></pre>"},{"location":"04-certificate-authority/#create-client-and-server-certificates","title":"Create Client and Server Certificates","text":"<p>In this section you will generate client and server certificates for each Kubernetes component and a client certificate for the Kubernetes <code>admin</code> user.</p> <p>Generate the certificates and private keys:</p> <pre><code>certs=(\n  \"admin\" \"node-0\" \"node-1\"\n  \"kube-proxy\" \"kube-scheduler\"\n  \"kube-controller-manager\"\n  \"kube-api-server\"\n  \"service-accounts\"\n)\n</code></pre> <pre><code>for i in ${certs[*]}; do\n  openssl genrsa -out \"${i}.key\" 4096\n\n  openssl req -new -key \"${i}.key\" -sha256 \\\n    -config \"ca.conf\" -section ${i} \\\n    -out \"${i}.csr\"\n\n  openssl x509 -req -days 3653 -in \"${i}.csr\" \\\n    -copy_extensions copyall \\\n    -sha256 -CA \"ca.crt\" \\\n    -CAkey \"ca.key\" \\\n    -CAcreateserial \\\n    -out \"${i}.crt\"\ndone\n</code></pre> <p>The results of running the above command will generate a private key, certificate request, and signed SSL certificate for each of the Kubernetes components. You can list the generated files with the following command:</p> <pre><code>ls -1 *.crt *.key *.csr\n</code></pre>"},{"location":"04-certificate-authority/#distribute-the-client-and-server-certificates","title":"Distribute the Client and Server Certificates","text":"<p>In this section you will copy the various certificates to each machine under a directory that each Kubernetes components will search for the certificate pair. In a real-world environment these certificates should be treated like a set of sensitive secrets as they are often used as credentials by the Kubernetes components to authenticate to each other.</p> <p>Copy the appropriate certificates and private keys to the <code>node-0</code> and <code>node-1</code> machines:</p> <pre><code>for host in node-0 node-1; do\n  ssh root@$host mkdir /var/lib/kubelet/\n\n  scp ca.crt root@$host:/var/lib/kubelet/\n\n  scp $host.crt \\\n    root@$host:/var/lib/kubelet/kubelet.crt\n\n  scp $host.key \\\n    root@$host:/var/lib/kubelet/kubelet.key\ndone\n</code></pre> <p>Copy the appropriate certificates and private keys to the <code>server</code> machine:</p> <pre><code>scp \\\n  ca.key ca.crt \\\n  kube-api-server.key kube-api-server.crt \\\n  service-accounts.key service-accounts.crt \\\n  root@server:~/\n</code></pre> <p>The <code>kube-proxy</code>, <code>kube-controller-manager</code>, <code>kube-scheduler</code>, and <code>kubelet</code> client certificates will be used to generate client authentication configuration files in the next lab.</p> <p>Next: Generating Kubernetes Configuration Files for Authentication</p>"},{"location":"05-kubernetes-configuration-files/","title":"Generating Kubernetes Configuration Files for Authentication","text":"<p>In this lab you will generate Kubernetes configuration files, also known as kubeconfigs, which enable Kubernetes clients to locate and authenticate to the Kubernetes API Servers.</p>"},{"location":"05-kubernetes-configuration-files/#client-authentication-configs","title":"Client Authentication Configs","text":"<p>In this section you will generate kubeconfig files for the <code>kubelet</code> and the <code>admin</code> user.</p>"},{"location":"05-kubernetes-configuration-files/#the-kubelet-kubernetes-configuration-file","title":"The kubelet Kubernetes Configuration File","text":"<p>When generating kubeconfig files for Kubelets the client certificate matching the Kubelet's node name must be used. This will ensure Kubelets are properly authorized by the Kubernetes Node Authorizer.</p> <p>The following commands must be run in the same directory used to generate the SSL certificates during the Generating TLS Certificates lab.</p> <p>Generate a kubeconfig file the node-0 worker node:</p> <pre><code>for host in node-0 node-1; do\n  kubectl config set-cluster kubernetes-the-hard-way \\\n    --certificate-authority=ca.crt \\\n    --embed-certs=true \\\n    --server=https://server.kubernetes.local:6443 \\\n    --kubeconfig=${host}.kubeconfig\n\n  kubectl config set-credentials system:node:${host} \\\n    --client-certificate=${host}.crt \\\n    --client-key=${host}.key \\\n    --embed-certs=true \\\n    --kubeconfig=${host}.kubeconfig\n\n  kubectl config set-context default \\\n    --cluster=kubernetes-the-hard-way \\\n    --user=system:node:${host} \\\n    --kubeconfig=${host}.kubeconfig\n\n  kubectl config use-context default \\\n    --kubeconfig=${host}.kubeconfig\ndone\n</code></pre> <p>Results:</p> <pre><code>node-0.kubeconfig\nnode-1.kubeconfig\n</code></pre>"},{"location":"05-kubernetes-configuration-files/#the-kube-proxy-kubernetes-configuration-file","title":"The kube-proxy Kubernetes Configuration File","text":"<p>Generate a kubeconfig file for the <code>kube-proxy</code> service:</p> <pre><code>{\n  kubectl config set-cluster kubernetes-the-hard-way \\\n    --certificate-authority=ca.crt \\\n    --embed-certs=true \\\n    --server=https://server.kubernetes.local:6443 \\\n    --kubeconfig=kube-proxy.kubeconfig\n\n  kubectl config set-credentials system:kube-proxy \\\n    --client-certificate=kube-proxy.crt \\\n    --client-key=kube-proxy.key \\\n    --embed-certs=true \\\n    --kubeconfig=kube-proxy.kubeconfig\n\n  kubectl config set-context default \\\n    --cluster=kubernetes-the-hard-way \\\n    --user=system:kube-proxy \\\n    --kubeconfig=kube-proxy.kubeconfig\n\n  kubectl config use-context default \\\n    --kubeconfig=kube-proxy.kubeconfig\n}\n</code></pre> <p>Results:</p> <pre><code>kube-proxy.kubeconfig\n</code></pre>"},{"location":"05-kubernetes-configuration-files/#the-kube-controller-manager-kubernetes-configuration-file","title":"The kube-controller-manager Kubernetes Configuration File","text":"<p>Generate a kubeconfig file for the <code>kube-controller-manager</code> service:</p> <pre><code>{\n  kubectl config set-cluster kubernetes-the-hard-way \\\n    --certificate-authority=ca.crt \\\n    --embed-certs=true \\\n    --server=https://server.kubernetes.local:6443 \\\n    --kubeconfig=kube-controller-manager.kubeconfig\n\n  kubectl config set-credentials system:kube-controller-manager \\\n    --client-certificate=kube-controller-manager.crt \\\n    --client-key=kube-controller-manager.key \\\n    --embed-certs=true \\\n    --kubeconfig=kube-controller-manager.kubeconfig\n\n  kubectl config set-context default \\\n    --cluster=kubernetes-the-hard-way \\\n    --user=system:kube-controller-manager \\\n    --kubeconfig=kube-controller-manager.kubeconfig\n\n  kubectl config use-context default \\\n    --kubeconfig=kube-controller-manager.kubeconfig\n}\n</code></pre> <p>Results:</p> <pre><code>kube-controller-manager.kubeconfig\n</code></pre>"},{"location":"05-kubernetes-configuration-files/#the-kube-scheduler-kubernetes-configuration-file","title":"The kube-scheduler Kubernetes Configuration File","text":"<p>Generate a kubeconfig file for the <code>kube-scheduler</code> service:</p> <pre><code>{\n  kubectl config set-cluster kubernetes-the-hard-way \\\n    --certificate-authority=ca.crt \\\n    --embed-certs=true \\\n    --server=https://server.kubernetes.local:6443 \\\n    --kubeconfig=kube-scheduler.kubeconfig\n\n  kubectl config set-credentials system:kube-scheduler \\\n    --client-certificate=kube-scheduler.crt \\\n    --client-key=kube-scheduler.key \\\n    --embed-certs=true \\\n    --kubeconfig=kube-scheduler.kubeconfig\n\n  kubectl config set-context default \\\n    --cluster=kubernetes-the-hard-way \\\n    --user=system:kube-scheduler \\\n    --kubeconfig=kube-scheduler.kubeconfig\n\n  kubectl config use-context default \\\n    --kubeconfig=kube-scheduler.kubeconfig\n}\n</code></pre> <p>Results:</p> <pre><code>kube-scheduler.kubeconfig\n</code></pre>"},{"location":"05-kubernetes-configuration-files/#the-admin-kubernetes-configuration-file","title":"The admin Kubernetes Configuration File","text":"<p>Generate a kubeconfig file for the <code>admin</code> user:</p> <pre><code>{\n  kubectl config set-cluster kubernetes-the-hard-way \\\n    --certificate-authority=ca.crt \\\n    --embed-certs=true \\\n    --server=https://127.0.0.1:6443 \\\n    --kubeconfig=admin.kubeconfig\n\n  kubectl config set-credentials admin \\\n    --client-certificate=admin.crt \\\n    --client-key=admin.key \\\n    --embed-certs=true \\\n    --kubeconfig=admin.kubeconfig\n\n  kubectl config set-context default \\\n    --cluster=kubernetes-the-hard-way \\\n    --user=admin \\\n    --kubeconfig=admin.kubeconfig\n\n  kubectl config use-context default \\\n    --kubeconfig=admin.kubeconfig\n}\n</code></pre> <p>Results:</p> <pre><code>admin.kubeconfig\n</code></pre>"},{"location":"05-kubernetes-configuration-files/#distribute-the-kubernetes-configuration-files","title":"Distribute the Kubernetes Configuration Files","text":"<p>Copy the <code>kubelet</code> and <code>kube-proxy</code> kubeconfig files to the node-0 instance:</p> <pre><code>for host in node-0 node-1; do\n  ssh root@$host \"mkdir /var/lib/{kube-proxy,kubelet}\"\n\n  scp kube-proxy.kubeconfig \\\n    root@$host:/var/lib/kube-proxy/kubeconfig \\\n\n  scp ${host}.kubeconfig \\\n    root@$host:/var/lib/kubelet/kubeconfig\ndone\n</code></pre> <p>Copy the <code>kube-controller-manager</code> and <code>kube-scheduler</code> kubeconfig files to the controller instance:</p> <pre><code>scp admin.kubeconfig \\\n  kube-controller-manager.kubeconfig \\\n  kube-scheduler.kubeconfig \\\n  root@server:~/\n</code></pre> <p>Next: Generating the Data Encryption Config and Key</p>"},{"location":"06-data-encryption-keys/","title":"Generating the Data Encryption Config and Key","text":"<p>Kubernetes stores a variety of data including cluster state, application configurations, and secrets. Kubernetes supports the ability to encrypt cluster data at rest.</p> <p>In this lab you will generate an encryption key and an encryption config suitable for encrypting Kubernetes Secrets.</p>"},{"location":"06-data-encryption-keys/#the-encryption-key","title":"The Encryption Key","text":"<p>Generate an encryption key:</p> <pre><code>export ENCRYPTION_KEY=$(head -c 32 /dev/urandom | base64)\n</code></pre>"},{"location":"06-data-encryption-keys/#the-encryption-config-file","title":"The Encryption Config File","text":"<p>Create the <code>encryption-config.yaml</code> encryption config file:</p> <pre><code>envsubst &lt; configs/encryption-config.yaml \\\n  &gt; encryption-config.yaml\n</code></pre> <p>Copy the <code>encryption-config.yaml</code> encryption config file to each controller instance:</p> <pre><code>scp encryption-config.yaml root@server:~/\n</code></pre> <p>Next: Bootstrapping the etcd Cluster</p>"},{"location":"07-bootstrapping-etcd/","title":"Bootstrapping the etcd Cluster","text":"<p>Kubernetes components are stateless and store cluster state in etcd. In this lab you will bootstrap a three node etcd cluster and configure it for high availability and secure remote access.</p>"},{"location":"07-bootstrapping-etcd/#prerequisites","title":"Prerequisites","text":"<p>Copy <code>etcd</code> binaries and systemd unit files to the <code>server</code> instance:</p> <pre><code>scp \\\n  downloads/etcd-v3.4.27-linux-arm64.tar.gz \\\n  units/etcd.service \\\n  root@server:~/\n</code></pre> <p>The commands in this lab must be run on the <code>server</code> machine. Login to the <code>server</code> machine using the <code>ssh</code> command. Example:</p> <pre><code>ssh root@server\n</code></pre>"},{"location":"07-bootstrapping-etcd/#bootstrapping-an-etcd-cluster","title":"Bootstrapping an etcd Cluster","text":""},{"location":"07-bootstrapping-etcd/#install-the-etcd-binaries","title":"Install the etcd Binaries","text":"<p>Extract and install the <code>etcd</code> server and the <code>etcdctl</code> command line utility:</p> <pre><code>{\n  tar -xvf etcd-v3.4.27-linux-arm64.tar.gz\n  mv etcd-v3.4.27-linux-arm64/etcd* /usr/local/bin/\n}\n</code></pre>"},{"location":"07-bootstrapping-etcd/#configure-the-etcd-server","title":"Configure the etcd Server","text":"<pre><code>{\n  mkdir -p /etc/etcd /var/lib/etcd\n  chmod 700 /var/lib/etcd\n  cp ca.crt kube-api-server.key kube-api-server.crt \\\n    /etc/etcd/\n}\n</code></pre> <p>Each etcd member must have a unique name within an etcd cluster. Set the etcd name to match the hostname of the current compute instance:</p> <p>Create the <code>etcd.service</code> systemd unit file:</p> <pre><code>mv etcd.service /etc/systemd/system/\n</code></pre>"},{"location":"07-bootstrapping-etcd/#start-the-etcd-server","title":"Start the etcd Server","text":"<pre><code>{\n  systemctl daemon-reload\n  systemctl enable etcd\n  systemctl start etcd\n}\n</code></pre>"},{"location":"07-bootstrapping-etcd/#verification","title":"Verification","text":"<p>List the etcd cluster members:</p> <pre><code>etcdctl member list\n</code></pre> <pre><code>6702b0a34e2cfd39, started, controller, http://127.0.0.1:2380, http://127.0.0.1:2379, false\n</code></pre> <p>Next: Bootstrapping the Kubernetes Control Plane</p>"},{"location":"08-bootstrapping-kubernetes-controllers/","title":"Bootstrapping the Kubernetes Control Plane","text":"<p>In this lab you will bootstrap the Kubernetes control plane. The following components will be installed the controller machine: Kubernetes API Server, Scheduler, and Controller Manager.</p>"},{"location":"08-bootstrapping-kubernetes-controllers/#prerequisites","title":"Prerequisites","text":"<p>Copy Kubernetes binaries and systemd unit files to the <code>server</code> instance:</p> <pre><code>scp \\\n  downloads/kube-apiserver \\\n  downloads/kube-controller-manager \\\n  downloads/kube-scheduler \\\n  downloads/kubectl \\\n  units/kube-apiserver.service \\\n  units/kube-controller-manager.service \\\n  units/kube-scheduler.service \\\n  configs/kube-scheduler.yaml \\\n  configs/kube-apiserver-to-kubelet.yaml \\\n  root@server:~/\n</code></pre> <p>The commands in this lab must be run on the controller instance: <code>server</code>. Login to the controller instance using the <code>ssh</code> command. Example:</p> <pre><code>ssh root@server\n</code></pre>"},{"location":"08-bootstrapping-kubernetes-controllers/#provision-the-kubernetes-control-plane","title":"Provision the Kubernetes Control Plane","text":"<p>Create the Kubernetes configuration directory:</p> <pre><code>mkdir -p /etc/kubernetes/config\n</code></pre>"},{"location":"08-bootstrapping-kubernetes-controllers/#install-the-kubernetes-controller-binaries","title":"Install the Kubernetes Controller Binaries","text":"<p>Install the Kubernetes binaries:</p> <pre><code>{\n  chmod +x kube-apiserver \\\n    kube-controller-manager \\\n    kube-scheduler kubectl\n\n  mv kube-apiserver \\\n    kube-controller-manager \\\n    kube-scheduler kubectl \\\n    /usr/local/bin/\n}\n</code></pre>"},{"location":"08-bootstrapping-kubernetes-controllers/#configure-the-kubernetes-api-server","title":"Configure the Kubernetes API Server","text":"<pre><code>{\n  mkdir -p /var/lib/kubernetes/\n\n  mv ca.crt ca.key \\\n    kube-api-server.key kube-api-server.crt \\\n    service-accounts.key service-accounts.crt \\\n    encryption-config.yaml \\\n    /var/lib/kubernetes/\n}\n</code></pre> <p>Create the <code>kube-apiserver.service</code> systemd unit file:</p> <pre><code>mv kube-apiserver.service \\\n  /etc/systemd/system/kube-apiserver.service\n</code></pre>"},{"location":"08-bootstrapping-kubernetes-controllers/#configure-the-kubernetes-controller-manager","title":"Configure the Kubernetes Controller Manager","text":"<p>Move the <code>kube-controller-manager</code> kubeconfig into place:</p> <pre><code>mv kube-controller-manager.kubeconfig /var/lib/kubernetes/\n</code></pre> <p>Create the <code>kube-controller-manager.service</code> systemd unit file:</p> <pre><code>mv kube-controller-manager.service /etc/systemd/system/\n</code></pre>"},{"location":"08-bootstrapping-kubernetes-controllers/#configure-the-kubernetes-scheduler","title":"Configure the Kubernetes Scheduler","text":"<p>Move the <code>kube-scheduler</code> kubeconfig into place:</p> <pre><code>mv kube-scheduler.kubeconfig /var/lib/kubernetes/\n</code></pre> <p>Create the <code>kube-scheduler.yaml</code> configuration file:</p> <pre><code>mv kube-scheduler.yaml /etc/kubernetes/config/\n</code></pre> <p>Create the <code>kube-scheduler.service</code> systemd unit file:</p> <pre><code>mv kube-scheduler.service /etc/systemd/system/\n</code></pre>"},{"location":"08-bootstrapping-kubernetes-controllers/#start-the-controller-services","title":"Start the Controller Services","text":"<pre><code>{\n  systemctl daemon-reload\n\n  systemctl enable kube-apiserver \\\n    kube-controller-manager kube-scheduler\n\n  systemctl start kube-apiserver \\\n    kube-controller-manager kube-scheduler\n}\n</code></pre> <p>Allow up to 10 seconds for the Kubernetes API Server to fully initialize.</p>"},{"location":"08-bootstrapping-kubernetes-controllers/#verification","title":"Verification","text":"<pre><code>kubectl cluster-info \\\n  --kubeconfig admin.kubeconfig\n</code></pre> <pre><code>Kubernetes control plane is running at https://127.0.0.1:6443\n</code></pre>"},{"location":"08-bootstrapping-kubernetes-controllers/#rbac-for-kubelet-authorization","title":"RBAC for Kubelet Authorization","text":"<p>In this section you will configure RBAC permissions to allow the Kubernetes API Server to access the Kubelet API on each worker node. Access to the Kubelet API is required for retrieving metrics, logs, and executing commands in pods.</p> <p>This tutorial sets the Kubelet <code>--authorization-mode</code> flag to <code>Webhook</code>. Webhook mode uses the SubjectAccessReview API to determine authorization.</p> <p>The commands in this section will affect the entire cluster and only need to be run on the controller node.</p> <pre><code>ssh root@server\n</code></pre> <p>Create the <code>system:kube-apiserver-to-kubelet</code> ClusterRole with permissions to access the Kubelet API and perform most common tasks associated with managing pods:</p> <pre><code>kubectl apply -f kube-apiserver-to-kubelet.yaml \\\n  --kubeconfig admin.kubeconfig\n</code></pre>"},{"location":"08-bootstrapping-kubernetes-controllers/#verification_1","title":"Verification","text":"<p>At this point the Kubernetes control plane is up and running. Run the following commands from the <code>jumpbox</code> machine to verify it's working:</p> <p>Make a HTTP request for the Kubernetes version info:</p> <pre><code>curl -k --cacert ca.crt https://server.kubernetes.local:6443/version\n</code></pre> <pre><code>{\n  \"major\": \"1\",\n  \"minor\": \"28\",\n  \"gitVersion\": \"v1.28.3\",\n  \"gitCommit\": \"a8a1abc25cad87333840cd7d54be2efaf31a3177\",\n  \"gitTreeState\": \"clean\",\n  \"buildDate\": \"2023-10-18T11:33:18Z\",\n  \"goVersion\": \"go1.20.10\",\n  \"compiler\": \"gc\",\n  \"platform\": \"linux/arm64\"\n}\n</code></pre> <p>Next: Bootstrapping the Kubernetes Worker Nodes</p>"},{"location":"09-bootstrapping-kubernetes-workers/","title":"Bootstrapping the Kubernetes Worker Nodes","text":"<p>In this lab you will bootstrap two Kubernetes worker nodes. The following components will be installed: runc, container networking plugins, containerd, kubelet, and kube-proxy.</p>"},{"location":"09-bootstrapping-kubernetes-workers/#prerequisites","title":"Prerequisites","text":"<p>Copy Kubernetes binaries and systemd unit files to each worker instance:</p> <pre><code>for host in node-0 node-1; do\n  SUBNET=$(grep $host machines.txt | cut -d \" \" -f 4)\n  sed \"s|SUBNET|$SUBNET|g\" \\\n    configs/10-bridge.conf &gt; 10-bridge.conf \n\n  sed \"s|SUBNET|$SUBNET|g\" \\\n    configs/kubelet-config.yaml &gt; kubelet-config.yaml\n\n  scp 10-bridge.conf kubelet-config.yaml \\\n  root@$host:~/\ndone\n</code></pre> <pre><code>for host in node-0 node-1; do\n  scp \\\n    downloads/runc.arm64 \\\n    downloads/crictl-v1.28.0-linux-arm.tar.gz \\\n    downloads/cni-plugins-linux-arm64-v1.3.0.tgz \\\n    downloads/containerd-1.7.8-linux-arm64.tar.gz \\\n    downloads/kubectl \\\n    downloads/kubelet \\\n    downloads/kube-proxy \\\n    configs/99-loopback.conf \\\n    configs/containerd-config.toml \\\n    configs/kubelet-config.yaml \\\n    configs/kube-proxy-config.yaml \\\n    units/containerd.service \\\n    units/kubelet.service \\\n    units/kube-proxy.service \\\n    root@$host:~/\ndone\n</code></pre> <p>The commands in this lab must be run on each worker instance: <code>node-0</code>, <code>node-1</code>. Login to the worker instance using the <code>ssh</code> command. Example:</p> <pre><code>ssh root@node-0\n</code></pre>"},{"location":"09-bootstrapping-kubernetes-workers/#provisioning-a-kubernetes-worker-node","title":"Provisioning a Kubernetes Worker Node","text":"<p>Install the OS dependencies:</p> <pre><code>{\n  apt-get update\n  apt-get -y install socat conntrack ipset\n}\n</code></pre> <p>The socat binary enables support for the <code>kubectl port-forward</code> command.</p>"},{"location":"09-bootstrapping-kubernetes-workers/#disable-swap","title":"Disable Swap","text":"<p>By default, the kubelet will fail to start if swap is enabled. It is recommended that swap be disabled to ensure Kubernetes can provide proper resource allocation and quality of service.</p> <p>Verify if swap is enabled:</p> <pre><code>swapon --show\n</code></pre> <p>If output is empty then swap is not enabled. If swap is enabled run the following command to disable swap immediately:</p> <pre><code>swapoff -a\n</code></pre> <p>To ensure swap remains off after reboot consult your Linux distro documentation.</p> <p>Create the installation directories:</p> <pre><code>mkdir -p \\\n  /etc/cni/net.d \\\n  /opt/cni/bin \\\n  /var/lib/kubelet \\\n  /var/lib/kube-proxy \\\n  /var/lib/kubernetes \\\n  /var/run/kubernetes\n</code></pre> <p>Install the worker binaries:</p> <pre><code>{\n  mkdir -p containerd\n  tar -xvf crictl-v1.28.0-linux-arm.tar.gz\n  tar -xvf containerd-1.7.8-linux-arm64.tar.gz -C containerd\n  tar -xvf cni-plugins-linux-arm64-v1.3.0.tgz -C /opt/cni/bin/\n  mv runc.arm64 runc\n  chmod +x crictl kubectl kube-proxy kubelet runc \n  mv crictl kubectl kube-proxy kubelet runc /usr/local/bin/\n  mv containerd/bin/* /bin/\n}\n</code></pre>"},{"location":"09-bootstrapping-kubernetes-workers/#configure-cni-networking","title":"Configure CNI Networking","text":"<p>Create the <code>bridge</code> network configuration file:</p> <pre><code>mv 10-bridge.conf 99-loopback.conf /etc/cni/net.d/\n</code></pre>"},{"location":"09-bootstrapping-kubernetes-workers/#configure-containerd","title":"Configure containerd","text":"<p>Install the <code>containerd</code> configuration files:</p> <pre><code>{\n  mkdir -p /etc/containerd/\n  mv containerd-config.toml /etc/containerd/config.toml\n  mv containerd.service /etc/systemd/system/\n}\n</code></pre>"},{"location":"09-bootstrapping-kubernetes-workers/#configure-the-kubelet","title":"Configure the Kubelet","text":"<p>Create the <code>kubelet-config.yaml</code> configuration file:</p> <pre><code>{\n  mv kubelet-config.yaml /var/lib/kubelet/\n  mv kubelet.service /etc/systemd/system/\n}\n</code></pre>"},{"location":"09-bootstrapping-kubernetes-workers/#configure-the-kubernetes-proxy","title":"Configure the Kubernetes Proxy","text":"<pre><code>{\n  mv kube-proxy-config.yaml /var/lib/kube-proxy/\n  mv kube-proxy.service /etc/systemd/system/\n}\n</code></pre>"},{"location":"09-bootstrapping-kubernetes-workers/#start-the-worker-services","title":"Start the Worker Services","text":"<pre><code>{\n  systemctl daemon-reload\n  systemctl enable containerd kubelet kube-proxy\n  systemctl start containerd kubelet kube-proxy\n}\n</code></pre>"},{"location":"09-bootstrapping-kubernetes-workers/#verification","title":"Verification","text":"<p>The compute instances created in this tutorial will not have permission to complete this section. Run the following commands from the <code>jumpbox</code> machine.</p> <p>List the registered Kubernetes nodes:</p> <pre><code>ssh root@server \\\n  \"kubectl get nodes \\\n  --kubeconfig admin.kubeconfig\"\n</code></pre> <pre><code>NAME     STATUS   ROLES    AGE    VERSION\nnode-0   Ready    &lt;none&gt;   1m     v1.28.3\nnode-1   Ready    &lt;none&gt;   10s    v1.28.3\n</code></pre> <p>Next: Configuring kubectl for Remote Access</p>"},{"location":"10-configuring-kubectl/","title":"Configuring kubectl for Remote Access","text":"<p>In this lab you will generate a kubeconfig file for the <code>kubectl</code> command line utility based on the <code>admin</code> user credentials.</p> <p>Run the commands in this lab from the <code>jumpbox</code> machine.</p>"},{"location":"10-configuring-kubectl/#the-admin-kubernetes-configuration-file","title":"The Admin Kubernetes Configuration File","text":"<p>Each kubeconfig requires a Kubernetes API Server to connect to.</p> <p>You should be able to ping <code>server.kubernetes.local</code> based on the <code>/etc/hosts</code> DNS entry from a previous lap.</p> <pre><code>curl -k --cacert ca.crt \\\n  https://server.kubernetes.local:6443/version\n</code></pre> <pre><code>{\n  \"major\": \"1\",\n  \"minor\": \"28\",\n  \"gitVersion\": \"v1.28.3\",\n  \"gitCommit\": \"a8a1abc25cad87333840cd7d54be2efaf31a3177\",\n  \"gitTreeState\": \"clean\",\n  \"buildDate\": \"2023-10-18T11:33:18Z\",\n  \"goVersion\": \"go1.20.10\",\n  \"compiler\": \"gc\",\n  \"platform\": \"linux/arm64\"\n}\n</code></pre> <p>Generate a kubeconfig file suitable for authenticating as the <code>admin</code> user:</p> <p><pre><code>{\n  kubectl config set-cluster kubernetes-the-hard-way \\\n    --certificate-authority=ca.crt \\\n    --embed-certs=true \\\n    --server=https://server.kubernetes.local:6443\n\n  kubectl config set-credentials admin \\\n    --client-certificate=admin.crt \\\n    --client-key=admin.key\n\n  kubectl config set-context kubernetes-the-hard-way \\\n    --cluster=kubernetes-the-hard-way \\\n    --user=admin\n\n  kubectl config use-context kubernetes-the-hard-way\n}\n</code></pre> The results of running the command above should create a kubeconfig file in the default location <code>~/.kube/config</code> used by the  <code>kubectl</code> commandline tool. This also means you can run the <code>kubectl</code> command without specifying a config.</p>"},{"location":"10-configuring-kubectl/#verification","title":"Verification","text":"<p>Check the version of the remote Kubernetes cluster:</p> <pre><code>kubectl version\n</code></pre> <pre><code>Client Version: v1.28.3\nKustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3\nServer Version: v1.28.3\n</code></pre> <p>List the nodes in the remote Kubernetes cluster:</p> <pre><code>kubectl get nodes\n</code></pre> <pre><code>NAME     STATUS   ROLES    AGE   VERSION\nnode-0   Ready    &lt;none&gt;   30m   v1.28.3\nnode-1   Ready    &lt;none&gt;   35m   v1.28.3\n</code></pre> <p>Next: Provisioning Pod Network Routes</p>"},{"location":"11-pod-network-routes/","title":"Provisioning Pod Network Routes","text":"<p>Pods scheduled to a node receive an IP address from the node's Pod CIDR range. At this point pods can not communicate with other pods running on different nodes due to missing network routes.</p> <p>In this lab you will create a route for each worker node that maps the node's Pod CIDR range to the node's internal IP address.</p> <p>There are other ways to implement the Kubernetes networking model.</p>"},{"location":"11-pod-network-routes/#the-routing-table","title":"The Routing Table","text":"<p>In this section you will gather the information required to create routes in the <code>kubernetes-the-hard-way</code> VPC network.</p> <p>Print the internal IP address and Pod CIDR range for each worker instance:</p> <pre><code>{\n  SERVER_IP=$(grep server machines.txt | cut -d \" \" -f 1)\n  NODE_0_IP=$(grep node-0 machines.txt | cut -d \" \" -f 1)\n  NODE_0_SUBNET=$(grep node-0 machines.txt | cut -d \" \" -f 4)\n  NODE_1_IP=$(grep node-1 machines.txt | cut -d \" \" -f 1)\n  NODE_1_SUBNET=$(grep node-1 machines.txt | cut -d \" \" -f 4)\n}\n</code></pre> <pre><code>ssh root@server &lt;&lt;EOF\n  ip route add ${NODE_0_SUBNET} via ${NODE_0_IP}\n  ip route add ${NODE_1_SUBNET} via ${NODE_1_IP}\nEOF\n</code></pre> <pre><code>ssh root@node-0 &lt;&lt;EOF\n  ip route add ${NODE_1_SUBNET} via ${NODE_1_IP}\nEOF\n</code></pre> <pre><code>ssh root@node-1 &lt;&lt;EOF\n  ip route add ${NODE_0_SUBNET} via ${NODE_0_IP}\nEOF\n</code></pre>"},{"location":"11-pod-network-routes/#verification","title":"Verification","text":"<pre><code>ssh root@server ip route\n</code></pre> <pre><code>default via XXX.XXX.XXX.XXX dev ens160 \n10.200.0.0/24 via XXX.XXX.XXX.XXX dev ens160 \n10.200.1.0/24 via XXX.XXX.XXX.XXX dev ens160 \nXXX.XXX.XXX.0/24 dev ens160 proto kernel scope link src XXX.XXX.XXX.XXX \n</code></pre> <pre><code>ssh root@node-0 ip route\n</code></pre> <pre><code>default via XXX.XXX.XXX.XXX dev ens160 \n10.200.1.0/24 via XXX.XXX.XXX.XXX dev ens160 \nXXX.XXX.XXX.0/24 dev ens160 proto kernel scope link src XXX.XXX.XXX.XXX \n</code></pre> <pre><code>ssh root@node-1 ip route\n</code></pre> <pre><code>default via XXX.XXX.XXX.XXX dev ens160 \n10.200.0.0/24 via XXX.XXX.XXX.XXX dev ens160 \nXXX.XXX.XXX.0/24 dev ens160 proto kernel scope link src XXX.XXX.XXX.XXX \n</code></pre> <p>Next: Smoke Test</p>"},{"location":"12-smoke-test/","title":"Smoke Test","text":"<p>In this lab you will complete a series of tasks to ensure your Kubernetes cluster is functioning correctly.</p>"},{"location":"12-smoke-test/#data-encryption","title":"Data Encryption","text":"<p>In this section you will verify the ability to encrypt secret data at rest.</p> <p>Create a generic secret:</p> <pre><code>kubectl create secret generic kubernetes-the-hard-way \\\n  --from-literal=\"mykey=mydata\"\n</code></pre> <p>Print a hexdump of the <code>kubernetes-the-hard-way</code> secret stored in etcd:</p> <pre><code>ssh root@server \\\n    'etcdctl get /registry/secrets/default/kubernetes-the-hard-way | hexdump -C'\n</code></pre> <pre><code>00000000  2f 72 65 67 69 73 74 72  79 2f 73 65 63 72 65 74  |/registry/secret|\n00000010  73 2f 64 65 66 61 75 6c  74 2f 6b 75 62 65 72 6e  |s/default/kubern|\n00000020  65 74 65 73 2d 74 68 65  2d 68 61 72 64 2d 77 61  |etes-the-hard-wa|\n00000030  79 0a 6b 38 73 3a 65 6e  63 3a 61 65 73 63 62 63  |y.k8s:enc:aescbc|\n00000040  3a 76 31 3a 6b 65 79 31  3a 9b 79 a5 b9 49 a2 77  |:v1:key1:.y..I.w|\n00000050  c0 6a c9 12 7c b4 c7 c4  64 41 37 97 4a 83 a9 c1  |.j..|...dA7.J...|\n00000060  4f 14 ae 73 ab b8 38 26  11 14 0a 40 b8 f3 0e 0a  |O..s..8&amp;...@....|\n00000070  f5 a7 a2 2c b6 35 b1 83  22 15 aa d0 dd 25 11 3e  |...,.5..\"....%.&gt;|\n00000080  c4 e9 69 1c 10 7a 9d f7  dc 22 28 89 2c 83 dd 0b  |..i..z...\"(.,...|\n00000090  a4 5f 3a 93 0f ff 1f f8  bc 97 43 0e e5 05 5d f9  |._:.......C...].|\n000000a0  ef 88 02 80 49 81 f1 58  b0 48 39 19 14 e1 b1 34  |....I..X.H9....4|\n000000b0  f6 b0 9b 0a 9c 53 27 2b  23 b9 e6 52 b4 96 81 70  |.....S'+#..R...p|\n000000c0  a7 b6 7b 4f 44 d4 9c 07  51 a3 1b 22 96 4c 24 6c  |..{OD...Q..\".L$l|\n000000d0  44 6c db 53 f5 31 e6 3f  15 7b 4c 23 06 c1 37 73  |Dl.S.1.?.{L#..7s|\n000000e0  e1 97 8e 4e 1a 2e 2c 1a  da 85 c3 ff 42 92 d0 f1  |...N..,.....B...|\n000000f0  87 b8 39 89 e8 46 2e b3  56 68 41 b8 1e 29 3d ba  |..9..F..VhA..)=.|\n00000100  dd d8 27 4c 7f d5 fe 97  3c a3 92 e9 3d ae 47 ee  |..'L....&lt;...=.G.|\n00000110  24 6a 0b 7c ac b8 28 e6  25 a6 ce 04 80 ee c2 eb  |$j.|..(.%.......|\n00000120  4c 86 fa 70 66 13 63 59  03 c2 70 57 8b fb a1 d6  |L..pf.cY..pW....|\n00000130  f2 58 08 84 43 f3 70 7f  ad d8 30 63 3e ef ff b6  |.X..C.p...0c&gt;...|\n00000140  b2 06 c3 45 c5 d8 89 d3  47 4a 72 ca 20 9b cf b5  |...E....GJr. ...|\n00000150  4b 3d 6d b4 58 ae 42 4b  7f 0a                    |K=m.X.BK..|\n0000015a\n</code></pre> <p>The etcd key should be prefixed with <code>k8s:enc:aescbc:v1:key1</code>, which indicates the <code>aescbc</code> provider was used to encrypt the data with the <code>key1</code> encryption key.</p>"},{"location":"12-smoke-test/#deployments","title":"Deployments","text":"<p>In this section you will verify the ability to create and manage Deployments.</p> <p>Create a deployment for the nginx web server:</p> <pre><code>kubectl create deployment nginx \\\n  --image=nginx:latest\n</code></pre> <p>List the pod created by the <code>nginx</code> deployment:</p> <pre><code>kubectl get pods -l app=nginx\n</code></pre> <pre><code>NAME                     READY   STATUS    RESTARTS   AGE\nnginx-56fcf95486-c8dnx   1/1     Running   0          8s\n</code></pre>"},{"location":"12-smoke-test/#port-forwarding","title":"Port Forwarding","text":"<p>In this section you will verify the ability to access applications remotely using port forwarding.</p> <p>Retrieve the full name of the <code>nginx</code> pod:</p> <pre><code>POD_NAME=$(kubectl get pods -l app=nginx \\\n  -o jsonpath=\"{.items[0].metadata.name}\")\n</code></pre> <p>Forward port <code>8080</code> on your local machine to port <code>80</code> of the <code>nginx</code> pod:</p> <pre><code>kubectl port-forward $POD_NAME 8080:80\n</code></pre> <pre><code>Forwarding from 127.0.0.1:8080 -&gt; 80\nForwarding from [::1]:8080 -&gt; 80\n</code></pre> <p>In a new terminal make an HTTP request using the forwarding address:</p> <pre><code>curl --head http://127.0.0.1:8080\n</code></pre> <pre><code>HTTP/1.1 200 OK\nServer: nginx/1.25.3\nDate: Sun, 29 Oct 2023 01:44:32 GMT\nContent-Type: text/html\nContent-Length: 615\nLast-Modified: Tue, 24 Oct 2023 13:46:47 GMT\nConnection: keep-alive\nETag: \"6537cac7-267\"\nAccept-Ranges: bytes\n</code></pre> <p>Switch back to the previous terminal and stop the port forwarding to the <code>nginx</code> pod:</p> <pre><code>Forwarding from 127.0.0.1:8080 -&gt; 80\nForwarding from [::1]:8080 -&gt; 80\nHandling connection for 8080\n^C\n</code></pre>"},{"location":"12-smoke-test/#logs","title":"Logs","text":"<p>In this section you will verify the ability to retrieve container logs.</p> <p>Print the <code>nginx</code> pod logs:</p> <pre><code>kubectl logs $POD_NAME\n</code></pre> <pre><code>...\n127.0.0.1 - - [01/Nov/2023:06:10:17 +0000] \"HEAD / HTTP/1.1\" 200 0 \"-\" \"curl/7.88.1\" \"-\"\n</code></pre>"},{"location":"12-smoke-test/#exec","title":"Exec","text":"<p>In this section you will verify the ability to execute commands in a container.</p> <p>Print the nginx version by executing the <code>nginx -v</code> command in the <code>nginx</code> container:</p> <pre><code>kubectl exec -ti $POD_NAME -- nginx -v\n</code></pre> <pre><code>nginx version: nginx/1.25.3\n</code></pre>"},{"location":"12-smoke-test/#services","title":"Services","text":"<p>In this section you will verify the ability to expose applications using a Service.</p> <p>Expose the <code>nginx</code> deployment using a NodePort service:</p> <pre><code>kubectl expose deployment nginx \\\n  --port 80 --type NodePort\n</code></pre> <p>The LoadBalancer service type can not be used because your cluster is not configured with cloud provider integration. Setting up cloud provider integration is out of scope for this tutorial.</p> <p>Retrieve the node port assigned to the <code>nginx</code> service:</p> <pre><code>NODE_PORT=$(kubectl get svc nginx \\\n  --output=jsonpath='{range .spec.ports[0]}{.nodePort}')\n</code></pre> <p>Make an HTTP request using the IP address and the <code>nginx</code> node port:</p> <pre><code>curl -I http://node-0:${NODE_PORT}\n</code></pre> <pre><code>HTTP/1.1 200 OK\nServer: nginx/1.25.3\nDate: Sun, 29 Oct 2023 05:11:15 GMT\nContent-Type: text/html\nContent-Length: 615\nLast-Modified: Tue, 24 Oct 2023 13:46:47 GMT\nConnection: keep-alive\nETag: \"6537cac7-267\"\nAccept-Ranges: bytes\n</code></pre> <p>Next: Cleaning Up</p>"},{"location":"13-cleanup/","title":"Cleaning Up","text":"<p>In this lab you will delete the compute resources created during this tutorial.</p>"},{"location":"13-cleanup/#compute-instances","title":"Compute Instances","text":"<p>Delete the controller and worker compute instances.</p>"}]}